Sentinel: 流量控制+流量整形+熔断降级+系统自适应保护

API网关出现的原因是微服务架构的出现，不同的微服务会有不同的网络地址，而外部客户端需要调用多个服务的接口才能完成一个业务需求，如果让客户端直接与各个微服务通信，会有以下问题：
	1. 客户端会多次请求不同的微服务，增加了客户端的复杂性；
	2. 存在跨域请求，在一定场景下处理相对复杂；
	3. 认证复杂，每个服务都需要独立认证；
	4. 难以重构，随着项目迭代，可能需要重新划分微服务。例如，多个服务合并成一个或者将一个拆分成多个。如果客户端直接与各个微服务通信，那么重构将很难实施；
	5. 某些微服务可能使用了防火墙/浏览器不友好的协议，直接访问有困难。

Spring Cloud Gateway features:
	1. Built on Spring Framework 5, Project Reactor and Spring Boot 2.0;
	2. Able to match routes on any request attribute.
	3. Predicates and filters are specific to routes.
	4. Hystrix Circuit Breaker integration.
	5. Spring Cloud DiscoveryClient integration.
	6. Easy to write Predicates and Filters.
	7. Request Rate Limiting;
	8. Path Rewriting.

Spring Cloud Gateway的核心：
	1. Route: The basic building block of the gateway. It is defined by an ID, a destination URI, a collection of predicates, and a collection of filters. A route is matched if the aggregate predicate is true.
	2. Predicate: This is a Java 8 Function Predicate. The input type is a Spring Framework ServerWebExchange. This lets you match on anything from the HTTP request, such as headers or parameters.
	3. Filter: There are instance of Spring Framework GatewayFilter that have been constructed with a specific factory. Here, you can modify requests and responses before or after sending the downstream request. 

Clients make requests to Spring Cloud Gateway. If the Gateway Handler Mapping determines that a request matches a route, it is sent to the Gateway Web Handler. This handler runs the request through a filter chain that is specific to the request. The reason the filters are divided by the dotted line is that filters can run logic both before and after the proxy request is sent. All "pre" filter logic is executed. Then the proxy request is made. After the proxy request is made, the "post" filter logic is run. 

内置的Route Predicate Factories:
	1. The After Route Predicate Factory;
	2. The Before Route Predicate Factory;
	3. The Between Route Predicate Factory;
	4. The Cookie Route Predicate Factory;
	5. The Header Route Predicate Factory;
	6. The Host Route Predicate Factory;
	7. The Method Route Predicate Factory;
	8. The Path Route Predicate Factory;
	9. The Query Route Predicate Factory;
	10. The RemoteAddr Route Predicate Factory;
	11. The Weight Route Predicate Factory;
--------------------------------------------------
The Redis RateLimiter:
A steady rate is accomplished by setting the same value in replenishRate and burstCapacity. Temporary bursts can be allowed by setting burstCapacity higher than replenishRate. In this case, the rate limiter needs to be allowed some time between bursts(according to replenishRate), as two consecutive bursts will result in dropped requests(HTTP 429 - Too Many Request). The following listing configures a redis-rate-limiter:
Rate limits bellow 1 request/s are accomplished by setting replenishRate to the wanted number of requests, requestedTokens to the timespan in seconds and burstCapacity to the product of replenishRate and requestedTokens, e.g. setting replenishRate=1, requestedTokens=60 and burstCapacity=60 will result in a limit of 1 request/min.
--------------------------------------------------
内置的GatewayFilter Factories:
	1. The AddRequestHeader GatewayFilter Factory;
	2. The AddRequestParameter GatewayFilter Factory;
	3. The AddResponseHeader GatewayFilter Factory;
	4. The DedupeResponseHeader GatewayFilter Factory;
	5. The Hystrix GatewayFilter Factory;
	6. Spring Cloud CircuitBreaker GatewayFilter Factory;
	7. The FallbackHeaders GatewayFilter Factory;
	8. The MapRequestHeader GatewayFilter;
	9. The PrefixPath GatewayFilter Factory;
	10. The PreserveHostHeader GatewayFilter Factory;
	11. The RequestRateLimiter GatewayFilter Factory;
	12. The RedirectTo GatewayFilter Factory;
	13. The RemoveRequestHeader GatewayFilter Factory;
	14. The RemoveResponseHeader GatewayFilter Factory;
	15. The RemoveRequestParameter GatewayFilter Factory;
	16. The RewritePath GatewayFilter Factory;
	17. The RewriteLocationResponseHeader GatewayFilter Factory;
	18. The RewriteResponseHeader GatewayFilter Factory;
	19. The SaveSession GatewayFilter Factory;
	20. The SecureHeaders GatewayFilter Factory;
	21. The SetPath GatewayFilter Factory;
	22. The SetRequestHeader GatewayFilter Factory;
	23. The SetResponseHeader GatewayFilter Factory;
	24. The SetStatus GatewayFilter Factory;
	25. The StripPrefix GatewayFilter Factory;
	26. The Retry GatewayFilter Factory;
	27. The RequestSize GatewayFilter Factory;
	28. Modify a Request Body GatewayFilter Factory;
	29. Modify a Response Body GatewayFilter Factory;
	30. Default Filters;

Zuul is a JVM-based router and server-side load balancer from Netflix. 
Netflix uses Zuul for the following:
	1. Authentication;
	2. Insights;
	3. Stress Testing;
	4. Canary Testing;
	5. Dynamic Routing;
	6. Service Migration;
	7. Load Shedding;
	8. Security;
	9. Static Response handling;
	10. Active traffic management;
Zuul's rule engine lets rules and filters be written in essentially any JVM language, with built-in support for Java and Groovy. 

注意：在网络分裂出现期间，客户端C1可以向主节点B发送写命令的最大时间是有限制的，这一时间限制称为节点超时时间(node timeout)，是Redis集群的一个重要配置选项。
开启集群的Redis节点的最小配置如下：
port 7000
cluster-enabled yes
cluster-config-file nodes.conf 
cluster-node-timeout 5000
appendonly yes 

用例图是指由参与者(Actor)、用例(Use Case)、边界以及它们之间的关系构成的用于描述系统功能的视图。用例图(Use Case)是外部用户所能观察到的系统功能的模型图。用例图是系统的蓝图。用例图呈现了一些参与者，一些用例，以及它们之间的关系，主要用于对系统、子系统或类的功能行为进行建模。

Zuul: If you would like to provide a default fallback for all routes, you can create a bean of type #FallbackProvider and have the #getRoute method return * or null, as shown in the following example:
class MyFallbackProvider implements FallbackProvider {
	@Override
	public String getRoute() {
		return "*";
	}
	
	@Override 
	public ClientHttpResponse fallbackResponse(String route, Throwable throwable) {
		return new ClientHttpResponse() {
			@Override
			public HttpStatus getStatusCode() throws IOException {
				return HttpStatus.OK;
			}
			@Override
            public int getRawStatusCode() throws IOException {
                return 200;
            }
            @Override
            public String getStatusText() throws IOException {
                return "OK";
            }
            @Override
            public void close() {

            }
            @Override
            public InputStream getBody() throws IOException {
                return new ByteArrayInputStream("fallback".getBytes());
            }
            @Override
            public HttpHeaders getHeaders() {
                HttpHeaders headers = new HttpHeaders();
                headers.setContentType(MediaType.APPLICATION_JSON);
                return headers;
            }
		};
	}
}

If Zuul is fronting a web application, you may need to re-write the Location header when the web application redirects through a HTTP status code of 3XX. Otherwise, the browser redirects to the web application's URL instead of the Zuul URL. You can configure a LocationRewriteFilter Zuul filter to re-write the Location header to the Zuul's URL. It also adds back the stripped golbal and route-specific prefixes. 

The Zuul Servlet: Zuul is implemented as a Servlet. For the general cases, Zuul is embedded into the Spring Dispatch mechanism. This lets Spring MVC be in control of the routing. In this case, Zuul buffers requests. If there is a need to go through Zuul without buffering requests(for example, for large file uploads), the Servlet is also installed outside of the Spring Dispatcher. By default, the servlet has an address of /zuul. This patch can be changed with the zuul.servlet-path property. 

To pass information between filters, Zuul uses a RequestContext. Its data is held in a ThreadLocal specific to each request. Information about where to route requests, errors, and the actual HttpServletRequest and HttpServletResponse are stored there. Thre RequestContext extends ConcurrentHashMap, so anything can be stored in the context. FilterConstants contains the keys used by the filters installed by Spring Cloud Netflix. 

Spring Cloud Netfilx installs a number of filters, depending on which annotation was used to enable Zuul. @EnableZuulProxy is a superset of @EnableZuulServer. In other words, @EnableZuulProxy contains all the filters installed by @EnableZuulServer. The additional filters int the "proxy" enable routing functionality. If you want a "blank" Zuul, you should use @EnableZuulServer. 

@EnableZuulServer Filters: 
@EnableZuulServer creates a SimpleRouteLocator that loads route definitions from Spring Boot configuration files. 
	1. Pre filters:
		1.1 ServletDetectionFilter
		1.2 FormBodyWrapperFilter
		1.3 DebugFilter
	2. Route filters:
		2.1 SendForwardFilter
	3. Post filters:
		3.1 SendResponseFilter
	4. Error filters:
		4.1 SendErrorFilter

@EnableZuulProxy Filters:
Creates a DiscoveryClientRouteLocator that loads route definitions from a DiscoveryClient(such as Eureka) as well as from properties. A route is created from each serviceId from the DiscoveryClient. As new services are added, the routes are refreshed.
In addition to the filters described earlier, the following filters are installed(as normal Spring Beans):
Pre filters:
	PreDecorationFilter
Route filters:
	RibbonRoutingFilter
	SimpleHostRoutingFilter

Zuul Eager ApplicationContext Loading:Zuul internally uses Ribbon for calling the remote URLs. By default, Ribbon clients are lazily loaded by Spring Cloud on first call. This behavior can be changed for Zuul by using configuration: zuul.ribbon.earger-load.enabled=true, which results eager loading of the child Ribbon related Application contexts at startup time. 

Spring Cloud Gateway Global Filters:
	1. Combined Global Filter and GatewayFilter Ordering;
	2. Forward Routing Filter;
	3. The LoadBalancerClient Filter;
	4. The ReactiveLoadBalancerClientFilter;
	5. The Netty Routing Filter;
	6. The Netty Write Response Filter;
	7. The RouteToRequestUrl Filter;
	8. The Websocket Routing Filter;
	9. The Gateway Metrics Filter;
	10. Marking An Exchange As Routed.
------------------------------------------------------------
Spring Cloud Gateway: 
In order to write a Route Predicate you will need to implement RoutePredicateFactory. There is an abstract class called AbstractRoutePredicateFactory which you can extend.
To write a GatewayFilter, you must implement GatewayFilterFactory. You can extend an abstract class called AbstractGatewayFilterFactory.
To write a custom global filter, you must implement GlobalFilter interface. This applies the filter to all requests. 
------------------------------------------------------------
public interface BeanFactory {
	String FACTORY_BEAN_PREFIX = "&";
	Object getBean(String name) throws BeansException;
	<T> T getBean(String name, Class<T> requiredType) throws BeansException;
	Object getBean(String name, Object... args) throws BeansException;
	<T> T getBean(Class<T> requiredType) throws BeansException;
	<T> T getBean(Class<T> requiredType, Object... args) throws BeansException;
	<T> ObjectProvider<T> getBeanProvider(Class<T> requiredType);
	<T> ObjectProvider<T> getBeanProvider(ResolvableType requiredType);
	boolean containsBean(String name);
	boolean isSingleton(String name) throws NoSuchBeanDefinitionException;
	boolean isPrototype(String name) throws NoSuchBeanDefinitionException;
	boolean isTypeMatch(String name, ResolableType typeToMatch) throws NoSuchBeanDefinitionException;
	boolean isTypeMatch(String name, Class<?> typeToMatch) throws NoSuchBeanDefinitionException;
	@Nullable
	Class<?> getType(String name) throws NoSuchBeanDefinitionException;
	String[] getAliases(String name);
}

public interface FactoryBean<T> {
	@Nullable
	T getObject() throws Exception;
	@Nullable 
	Class<?> getObjectType();
	default boolean isSingleton() {return true;}
}

BeanFactory和FactoryBean的区别:
BeanFactory是接口，提供了IOC最基本的实现，给具体的IOC容器提供了规范，也就是Spring IOC所遵守的最底层和最基本的编程规范。它的职责包括：实例化、定位、配置应用程序中的对象及简历这些对象间的依赖。BeanFactory是Factory,也就是容器工厂或对象工厂，在Spring中，所有的对象都是由BeanFactory管理的。在Spring代码中，BeanFactory只是接口，并不是IOC容器的具体实现，但是Spring给出了很多实现，比如DefaultListableBeanFactory, XmlBeanFactory, ApplicationContext等，都是附加了某种功能的实现。
但是对于FactoryBean而言，这个Bean不是简单的bean，它是能生产或装饰对象的bean。一般情况下，Spring通过反射机制利用<bean>的class属性指定实现类实例化Bean，在某些情况下，实例化Bean过程比较复杂，配置方式的灵活性受到限制，FactoryBean提供更加灵活的实现方式，FactoryBean在Bean的实现方式上，加了一个工厂模式，一个装饰者模式。FactoryBean接口对于Spring框架非常重要，Spring自身提供了70多种FactoryBean的实现。它们隐藏了实例化复杂Bean的细节，给上层应用带来了便利。

事务注解的注意事项：
	1. 一个功能是否需要事务，必须纳入设计、编码考虑。不能仅完成基本功能；
	2. 如果加了事务，必须做好测试(尽量触发异常、测试回滚)，确保事务生效。
	3. @Transactional的注意事项
		3.1 尽量不要在接口上用@Transactional，而要在具体类上使用@Transactional注解，否则注解可能无效；
		3.2 将@Transactional放在类级别上，会使得所有方法都有事务。故@Transactional应该放在方法级别上，不需要使用事务的方法，就不要放置事务，比如某些查询方法，否则对性能有影响；
		3.3 使用了@Transactional的方法，对同一个类里面的方法调用，@Transactional无效。
		3.4 使用@Transactional方法，只能是public, @Transactional注解的方法都是被外部其他类调用才有效，故只能是public。
	4. 正确的设置propagation, isolation, timeout, readOnly, rollbackFor, rollbackForClassName, noRollbackFor;

解决hash冲突的方法:
	1. 开放定址法(线性探测再散列/平方探测再散列/随机探测再散列)；
	2. 链地址法；
	3. 再hash法；
	4. 建立公共溢出区;

refresh()方法是ConfigurableApplicationContext接口提供的方法，refresh()方法的具体实现是由AbstractApplicationContext这个抽象类实现的，AbstractApplicationContext这个抽象类继承了DefaultResourceLoader，并且实现了接口ConfigurableAppplicationContext。

用过哪些RPC框架，讲讲他们优缺点：
	1. UP的Magpai;
	2. RMI;
	3. Dubbo;
	4. Spring Cloud Feign;
	5. Motan;
	6. gRPC；
	7. Hessian;
	8. WebService:两种常用实现:CXF和Axis2
	9. Apache Thrift
	
信号量模式从始至终都只有请求线程自身，是同步调用模式，不支持超时调用，不支持直接熔断，由于没有线程的切换，开销非常小。
线程池模式可以支持异步调用，支持超时调用，支持直接熔断，存在线程切换，开销大。
------------------------------------------------------------------------
hystrix:
  command: #用于控制HystrixCommand的行为
    default:
      execution:
        isolation:
          strategy: THREAD #控制HystrixCommand的隔离策略，THREAD->线程池隔离策略(默认)，SEMAPHORE->信号量隔离策略
          thread:
            timeoutInMilliseconds: 1000 #配置HystrixCommand执行的超时时间，执行超过该时间会进行服务降级处理
            interruptOnTimeout: true #配置HystrixCommand执行超时的时候是否要中断
            interruptOnCancel: true #配置HystrixCommand执行被取消的时候是否要中断
          timeout:
            enabled: true #配置HystrixCommand的执行是否启用超时时间
          semaphore:
            maxConcurrentRequests: 10 #当使用信号量隔离策略时，用来控制并发量的大小，超过该并发量的请求会被拒绝
      fallback:
        enabled: true #用于控制是否启用服务降级
      circuitBreaker: #用于控制HystrixCircuitBreaker的行为
        enabled: true #用于控制断路器是否跟踪健康状况以及熔断请求
        requestVolumeThreshold: 20 #超过该请求数的请求会被拒绝
        forceOpen: false #强制打开断路器，拒绝所有请求
        forceClosed: false #强制关闭断路器，接收所有请求
      requestCache:
        enabled: true #用于控制是否开启请求缓存
  collapser: #用于控制HystrixCollapser的执行行为
    default:
      maxRequestsInBatch: 100 #控制一次合并请求合并的最大请求数
      timerDelayinMilliseconds: 10 #控制多少毫秒内的请求会被合并成一个
      requestCache:
        enabled: true #控制合并请求是否开启缓存
  threadpool: #用于控制HystrixCommand执行所在线程池的行为
    default:
      coreSize: 10 #线程池的核心线程数
      maximumSize: 10 #线程池的最大线程数，超过该线程数的请求会被拒绝
      maxQueueSize: -1 #用于设置线程池的最大队列大小，-1采用SynchronousQueue，其他正数采用LinkedBlockingQueue
      queueSizeRejectionThreshold: 5 #用于设置线程池队列的拒绝阀值，由于LinkedBlockingQueue不能动态改版大小，使用时需要用该参数来控制线程数
 
实例配置
 
实例配置只需要将全局配置中的default换成与之对应的key即可。
hystrix:
  command:
    HystrixComandKey: #将default换成HystrixComrnandKey
      execution:
        isolation:
          strategy: THREAD
  collapser:
    HystrixCollapserKey: #将default换成HystrixCollapserKey
      maxRequestsInBatch: 100
  threadpool:
    HystrixThreadPoolKey: #将default换成HystrixThreadPoolKey
      coreSize: 10 
HystrixComandKey对应@HystrixCommand中的commandKey属性；
HystrixCollapserKey对应@HystrixCollapser注解中的collapserKey属性
------------------------------------------------------------------------
Hystrix配置的默认值是在HystrixCommandProperties, HystrixCollapserProperties, HystixThreadPoolProperties类。

To provide a default configuration for all of your circuit breakers create a Customizer bean that is passed a HystrixCircuitBreakerFactory or ReactiveHystrixCircuitBreakerFactory. The configureDefault method can be used to provide a default configuration. 
@Bean 
public Customizer<HystrixCircuitBreakerFactory> defaultConifg() {
	return factory -> factory.configureDefault(id -> HystrixCommand.Setter
			.withGroupKey(HystrixCommandGroupKey.Factory.asKey(id))
			.andCommandPropertiesDefaults(HystrixCommandProperties.Setter()
			.withExecutionTimeoutInMilliseconds(4000)));
}

HystrixObservableCommand
A service failure in the low level of services can cause cascading failure all the way up to the user. When calls to a particular service exceed circuitBreaker.requestVolumeThreshold(default: 20 requests) and the failure percentage is greater than circuitBreaker.errorThresholdPercentage(default:>50%) in a rolling window defined by metrics.rollingStats.timeInMilliseconds(default: 10 seconds), the circuit opens and the call is not made. In cases of error and an open circuit, a fallback can be provided by the developer. 

Looking at an individual instance’s Hystrix data is not very useful in terms of the overall health of the system. Turbine is an application that aggregates all of the relevant /hystrix.stream endpoints into a combined /turbine.stream for use in the Hystrix Dashboard. 

熔断器Hystrix最主要的作用:
	1. 资源隔离；
	2. 熔断；
	3. 降级；
	4. 限流；
	5. 缓存；
	6. 请求合并；

MySQL的各种高可用方案，大多是基于下面几种基础来部署的：
	1. 基于主从复制；
	2. 基于Galera协议；
	3. 基于NDB引擎；
	4. 基于中间件/proxy;
	5. 基于共享存储；
	6. 基于主机高可用；

Galera是Codership提供的多主数据同步复制机制，可以实现多个节点间的数据同步复制以及读写，并且可保障数据库的服务高可用及数据一致性。
基于Galera的高可用方案主要有MariaDB Galera Cluster和Percona XtraDB Cluster(简称PXC)，目前PXC用的比较多一些。
在PXC中，一次数据写入在各个节点间的验证/回滚流程。
PXC的优点：
	1. 服务高可用；
	2. 数据同步复制(并发复制)，几乎无延迟；
	3. 多个可同时读写节点，可实现写扩展，不过最好事先进行分库分表，让各个节点分别写不同的表或库，避免让Galera解决数据冲突；
	4. 新节点可以自动部署，部署操作简单；
	5. 数据严格一致性，尤其适合电商类应用；
	6. 完全兼容MySQL;
PXC的局限性：
	1. 只支持InnoDB引擎；
	2. 所有表都要有主键；
	3. 不支持Lock Table等显式锁操作；
	4. 锁冲突、死锁问题相对更多；
	5. 不支持XA；
	6. 集群吞吐量/性能取决于短板；
	7. 新加入节点采用SST时，代价高；
	8. 存在写扩大问题；
	9. 如果并发事务量很大的话，建议采用InfinBand网络，降低网络延迟；
事实上，采用PXC的主要目的是解决数据的一致性问题，高可用是顺带事先的。因为PXC存在写扩大以及短板效应，并发效率会有较大损失，类似于semi sync replication机制。

MySQL多主复制管理器(MMM)
MySQL异步复制有如下问题：
	1. slave服务器的数据集总是落后于master；
	2. MySQL复制很慢-它从二进制日志回访事务；
对于Galera，事务是在它们被提交之前被所有节点确认。如果一个事务在一个节点失败了，那个节点将立刻从集群移除。换句话说，Galera主从复制是同步点。永远不会丢失事务-没有延迟(而且Galera的基于行的复制大约要快5倍).

MHA Manager可以单独部署在一台独立的机器上管理多个master-slave集群，也可以部署在一台slave节点上。MHA Node运行在每台MySQL服务器上，MHA Manager会定时探测集群中的master节点，当master出现故障时，它可以自动将最新数据的slave提升为新的master，然后将所有其他的slave重新指向新的master。整个故障转移过程对应用程序完全透明。

/* A cancellable asynchronous computation. This class provides a base implementation of #Future, with methods to start and cancel a computation, query to see if the computation is complete, and retrieve the result of the computation. The result can only be retrieved when the computation has completed; the #get methods will block if the computation has not yet completed. Once the computation has completed, the computation cannot be restarted or cancelled (unless the computation is invoked using #runAndReset).
A #FutureTask can be used to wrap a #Callable or #Runnable object. Because #FutureTask implements #Runnable, a #FutureTask can be submitted to an #Executor for execution. 
In addition to serving as a standalone class, this class provides #protected functionality that may be useful when creating customized task classes. 
*/
public class FutureTask<V> implements RunableFuture<V> {
	/*
     * Revision notes: This differs from previous versions of this
     * class that relied on AbstractQueuedSynchronizer, mainly to
     * avoid surprising users about retaining interrupt status during
     * cancellation races. Sync control in the current design relies
     * on a "state" field updated via CAS to track completion, along
     * with a simple Treiber stack to hold waiting threads.
     *
     * Style note: As usual, we bypass overhead of using
     * AtomicXFieldUpdaters and instead directly use Unsafe intrinsics.
     */

    /**
     * The run state of this task, initially NEW.  The run state
     * transitions to a terminal state only in methods set,
     * setException, and cancel.  During completion, state may take on
     * transient values of COMPLETING (while outcome is being set) or
     * INTERRUPTING (only while interrupting the runner to satisfy a
     * cancel(true)). Transitions from these intermediate to final
     * states use cheaper ordered/lazy writes because values are unique
     * and cannot be further modified.
     *
     * Possible state transitions:
     * NEW -> COMPLETING -> NORMAL
     * NEW -> COMPLETING -> EXCEPTIONAL
     * NEW -> CANCELLED
     * NEW -> INTERRUPTING -> INTERRUPTED
     */
    private volatile int state;
    private static final int NEW          = 0;
    private static final int COMPLETING   = 1;
    private static final int NORMAL       = 2;
    private static final int EXCEPTIONAL  = 3;
    private static final int CANCELLED    = 4;
    private static final int INTERRUPTING = 5;
    private static final int INTERRUPTED  = 6;

    /** The underlying callable; nulled out after running */
    private Callable<V> callable;
    /** The result to return or exception to throw from get() */
    private Object outcome; // non-volatile, protected by state reads/writes
    /** The thread running the callable; CASed during run() */
    private volatile Thread runner;
    /** Treiber stack of waiting threads */
    private volatile WaitNode waiters;
	...
	
	public V get() throws InterruptedException, ExecutionException {
		int s = state;
		if (s <= COMPLETING)
			s = awaitDone(false, 0L);
		return report(s);
	}
	
	public V get(long timeout, TimeUnit unit)
        throws InterruptedException, ExecutionException, TimeoutException {
        if (unit == null)
            throw new NullPointerException();
        int s = state;
        if (s <= COMPLETING &&
            (s = awaitDone(true, unit.toNanos(timeout))) <= COMPLETING)
            throw new TimeoutException();
        return report(s);
    }
	
    /**
     * Protected method invoked when this task transitions to state
     * {@code isDone} (whether normally or via cancellation). The
     * default implementation does nothing.  Subclasses may override
     * this method to invoke completion callbacks or perform
     * bookkeeping. Note that you can query status inside the
     * implementation of this method to determine whether this task
     * has been cancelled.
     */	
	protected void done() {}
	
	/**
     * Sets the result of this future to the given value unless
     * this future has already been set or has been cancelled.
     *
     * <p>This method is invoked internally by the {@link #run} method
     * upon successful completion of the computation.
     *
     * @param v the value
     */
    protected void set(V v) {
        if (UNSAFE.compareAndSwapInt(this, stateOffset, NEW, COMPLETING)) {
            outcome = v;
            UNSAFE.putOrderedInt(this, stateOffset, NORMAL); // final state
            finishCompletion();
        }
    }
}

/* 
...
Keep-alive times
If the pool currently has more than corePoolSize threads, excess threads will be terminated if they have been idle for more than keepAliveTime(#getKeepAliveTime(TimeUnit)). This provides a means of reducing resource consumption when the pool is not being actively used. If the pool becomes more active later, new threads will be constructed. This parameter can also be changed dynamically using method #setKeepAliveTime(long, TimeUnit). Using a value of Long.MAX_VALUE effectively disable idle threads from ever terminating prior to shut down. By default, the keep-alive policy applies only when there are more than corePoolSize threads. But method #allowCoreThreadTimeout(boolean) can be used to apply this time-out policy to core threads as well, so long as the keepAliveTime value is non-zero.

* There are three general strategies for queuing:
 * <ol>
 *
 * <li> <em> Direct handoffs.</em> A good default choice for a work
 * queue is a {@link SynchronousQueue} that hands off tasks to threads
 * without otherwise holding them. Here, an attempt to queue a task
 * will fail if no threads are immediately available to run it, so a
 * new thread will be constructed. This policy avoids lockups when
 * handling sets of requests that might have internal dependencies.
 * Direct handoffs generally require unbounded maximumPoolSizes to
 * avoid rejection of new submitted tasks. This in turn admits the
 * possibility of unbounded thread growth when commands continue to
 * arrive on average faster than they can be processed.  </li>
 *
 * <li><em> Unbounded queues.</em> Using an unbounded queue (for
 * example a {@link LinkedBlockingQueue} without a predefined
 * capacity) will cause new tasks to wait in the queue when all
 * corePoolSize threads are busy. Thus, no more than corePoolSize
 * threads will ever be created. (And the value of the maximumPoolSize
 * therefore doesn't have any effect.)  This may be appropriate when
 * each task is completely independent of others, so tasks cannot
 * affect each others execution; for example, in a web page server.
 * While this style of queuing can be useful in smoothing out
 * transient bursts of requests, it admits the possibility of
 * unbounded work queue growth when commands continue to arrive on
 * average faster than they can be processed.  </li>
 *
 * <li><em>Bounded queues.</em> A bounded queue (for example, an
 * {@link ArrayBlockingQueue}) helps prevent resource exhaustion when
 * used with finite maximumPoolSizes, but can be more difficult to
 * tune and control.  Queue sizes and maximum pool sizes may be traded
 * off for each other: Using large queues and small pools minimizes
 * CPU usage, OS resources, and context-switching overhead, but can
 * lead to artificially low throughput.  If tasks frequently block (for
 * example if they are I/O bound), a system may be able to schedule
 * time for more threads than you otherwise allow. Use of small queues
 * generally requires larger pool sizes, which keeps CPUs busier but
 * may encounter unacceptable scheduling overhead, which also
 * decreases throughput.  </li>
 
 Rejected tasks 
 New tasks submitted in method #execute(Runnable) will be rejected when the Executor has been shut down, and also when the Executor uses finite bounds for both maximum threads and work queue capacity, and is saturated. In either case, the #execute method invokes the RejectedExecutionHandler#rejectedExecution(Runnable, ThreadPoolExecutor) method of its RejectedExecutionHandler. 
...
*/
public class ThreadPoolExecutor extends AbstractExecutorService {
	    /**
     * The main pool control state, ctl, is an atomic integer packing
     * two conceptual fields
     *   workerCount, indicating the effective number of threads
     *   runState,    indicating whether running, shutting down etc
     *
     * In order to pack them into one int, we limit workerCount to
     * (2^29)-1 (about 500 million) threads rather than (2^31)-1 (2
     * billion) otherwise representable. If this is ever an issue in
     * the future, the variable can be changed to be an AtomicLong,
     * and the shift/mask constants below adjusted. But until the need
     * arises, this code is a bit faster and simpler using an int.
     *
     * The workerCount is the number of workers that have been
     * permitted to start and not permitted to stop.  The value may be
     * transiently different from the actual number of live threads,
     * for example when a ThreadFactory fails to create a thread when
     * asked, and when exiting threads are still performing
     * bookkeeping before terminating. The user-visible pool size is
     * reported as the current size of the workers set.
     *
     * The runState provides the main lifecycle control, taking on values:
     *
     *   RUNNING:  Accept new tasks and process queued tasks
     *   SHUTDOWN: Don't accept new tasks, but process queued tasks
     *   STOP:     Don't accept new tasks, don't process queued tasks,
     *             and interrupt in-progress tasks
     *   TIDYING:  All tasks have terminated, workerCount is zero,
     *             the thread transitioning to state TIDYING
     *             will run the terminated() hook method
     *   TERMINATED: terminated() has completed
     *
     * The numerical order among these values matters, to allow
     * ordered comparisons. The runState monotonically increases over
     * time, but need not hit each state. The transitions are:
     *
     * RUNNING -> SHUTDOWN
     *    On invocation of shutdown(), perhaps implicitly in finalize()
     * (RUNNING or SHUTDOWN) -> STOP
     *    On invocation of shutdownNow()
     * SHUTDOWN -> TIDYING
     *    When both queue and pool are empty
     * STOP -> TIDYING
     *    When pool is empty
     * TIDYING -> TERMINATED
     *    When the terminated() hook method has completed
     *
     * Threads waiting in awaitTermination() will return when the
     * state reaches TERMINATED.
     *
     * Detecting the transition from SHUTDOWN to TIDYING is less
     * straightforward than you'd like because the queue may become
     * empty after non-empty and vice versa during SHUTDOWN state, but
     * we can only terminate if, after seeing that it is empty, we see
     * that workerCount is 0 (which sometimes entails a recheck -- see
     * below).
     */
    private final AtomicInteger ctl = new AtomicInteger(ctlOf(RUNNING, 0));
    private static final int COUNT_BITS = Integer.SIZE - 3;
    private static final int CAPACITY   = (1 << COUNT_BITS) - 1;

    // runState is stored in the high-order bits
    private static final int RUNNING    = -1 << COUNT_BITS;
    private static final int SHUTDOWN   =  0 << COUNT_BITS;
    private static final int STOP       =  1 << COUNT_BITS;
    private static final int TIDYING    =  2 << COUNT_BITS;
    private static final int TERMINATED =  3 << COUNT_BITS;

    // Packing and unpacking ctl
    private static int runStateOf(int c)     { return c & ~CAPACITY; }
    private static int workerCountOf(int c)  { return c & CAPACITY; }
    private static int ctlOf(int rs, int wc) { return rs | wc; }
	...
	/* Decrements the workerCount field of ctl. This is called only on abrupt termination of 
		a thread (see processWorkerExit). Other decrements are performed within getTask.
	*/
	private void decrementWorkerCount() {
		do {} while (! compareAndDecrementWorkerCount(ctl.get())));
	}
	...
	private final BlockingQueue<Runnable> workQueue;
	...
	/* Lock held on access to workers set and related bookkeeping.
		While we could use a concurrent set of some sort, it turns out to be 
		gengerally preferable to use a lock. Among the reasons is that this serializes
		interruptIdleWorkers, which avoids unnecessary interrupt storms, especially during shutdown.
		Otherwise exiting threads would concurrently interrupt those that have not yet interrupted.
		It also simplifies some of the associated statistics bookkeeping of largestPoolSize etc.
		We also hold mainLock on shutdown and shutdownNow, for the sake of ensuring workers set is 
		stable while separately checking permission to interrupt and actually interrupting.  
	*/
	private final ReetrantLock mainLock = new ReentrantLock();
	/**
     * Set containing all worker threads in pool. Accessed only when
     * holding mainLock.
     */
    private final HashSet<Worker> workers = new HashSet<Worker>();

    /**
     * Wait condition to support awaitTermination
     */
    private final Condition termination = mainLock.newCondition();

    /**
     * Tracks largest attained pool size. Accessed only under
     * mainLock.
     */
    private int largestPoolSize;

    /**
     * Counter for completed tasks. Updated only on termination of
     * worker threads. Accessed only under mainLock.
     */
    private long completedTaskCount;
	/* All user control parameters are declared as volatile so that 
		ongoing actions are based on freshest values, but without need for locking, 
		since no internal invariants depend on them changing synchronously with 
		respect to other actions.
	*/
	/**
     * Timeout in nanoseconds for idle threads waiting for work.
     * Threads use this timeout when there are more than corePoolSize
     * present or if allowCoreThreadTimeOut. Otherwise they wait
     * forever for new work.
     */
    private volatile long keepAliveTime;

    /**
     * If false (default), core threads stay alive even when idle.
     * If true, core threads use keepAliveTime to time out waiting
     * for work.
     */
    private volatile boolean allowCoreThreadTimeOut;

    /**
     * Core pool size is the minimum number of workers to keep alive
     * (and not allow to time out etc) unless allowCoreThreadTimeOut
     * is set, in which case the minimum is zero.
     */
    private volatile int corePoolSize;

    /**
     * Maximum pool size. Note that the actual maximum is internally
     * bounded by CAPACITY.
     */
    private volatile int maximumPoolSize;

    /**
     * The default rejected execution handler
     */
    private static final RejectedExecutionHandler defaultHandler =
        new AbortPolicy();
	...
	
	/* Class Worker mainly maintains interrupt control state for threads running tasks, 
		along with other minor bookkeeping. This class opportunistically extends AbstractQueuedSynchronizer 
		to simplify acquiring and releasing a lock surrounding each task execution. This protects against 
		interrupts that are intended to wake up a worker thread waiting for a task from instead 
		interrupting a task being run. We implement a simple non-reentrant mutual exclusion lock 
		rather than use ReentrantLock because we do not want worker tasks to be able to reacquire the lock 
		when they invoke pool control methods like setCorePoolSize. Additionally, to suppress interrupts 
		until the thread actually starts running tasks, we initialize lock state to a negative value, 
		and clear it upon start(in #runWorker).
	*/
	private final class Worker extends AbstractQueuedSynchronizer implements Runnable {
		...
	}
	
	final void runWorker(Worker w) {
		Thread wt = Thread.currentThread();
		Runnable task = w.firstTask;
		w.firstTask = null;
		w.unlock();   // allow interrupts 
		boolean completedAbruptly = true;
		try {
			while (task != null || (task = getTask()) != null) {
				w.lock();
				// If pool is stopping, ensure thread is interrupted;
				// if not, ensure thread is not interrupted. This requires a recheck 
				// in second case to deal with shutdownNow race while clearing interrupt 
				if ((runStateAtLeast(ctl.get(), STOP) || 
					(Thread.interrupted() && runStateAtLeast(ctl.get(), STOP))) 
						&& !wt.isInterrupted())
					wt.interrupt();
				try {
					beforeExecute(wt, task);
					Throwable thrown = null;
					try {
						task.run();
					} catch (RuntimeException x) {
						thrown = x; throw x;
					} catch (Error x) {
						thrown = x; throw x;
					} catch (Throwable x) {
						thrown = x; throw new Error(x);
					} finally {
						afterExecute(task, thrown);
					}
				} finally {
					task = null;
					w.completedTasks ++;
					w.unlock();
				}
			}
			completedAbruptly = false;
		} finally {
			processWorkerExit(w, completedAbruptly);
		}
	}
	...
	/**
     * Sets the core number of threads.  This overrides any value set
     * in the constructor.  If the new value is smaller than the
     * current value, excess existing threads will be terminated when
     * they next become idle.  If larger, new threads will, if needed,
     * be started to execute any queued tasks.
     *
     * @param corePoolSize the new core size
     * @throws IllegalArgumentException if {@code corePoolSize < 0}
     * @see #getCorePoolSize
     */
    public void setCorePoolSize(int corePoolSize) {
        if (corePoolSize < 0)
            throw new IllegalArgumentException();
        int delta = corePoolSize - this.corePoolSize;
        this.corePoolSize = corePoolSize;
        if (workerCountOf(ctl.get()) > corePoolSize)
            interruptIdleWorkers();
        else if (delta > 0) {
            // We don't really know how many new threads are "needed".
            // As a heuristic, prestart enough new workers (up to new
            // core size) to handle the current number of tasks in
            // queue, but stop if queue becomes empty while doing so.
            int k = Math.min(delta, workQueue.size());
            while (k-- > 0 && addWorker(null, true)) {
                if (workQueue.isEmpty())
                    break;
            }
        }
    }
	...
	/**
     * Starts a core thread, causing it to idly wait for work. This
     * overrides the default policy of starting core threads only when
     * new tasks are executed. This method will return {@code false}
     * if all core threads have already been started.
     *
     * @return {@code true} if a thread was started
     */
    public boolean prestartCoreThread() {
        return workerCountOf(ctl.get()) < corePoolSize &&
            addWorker(null, true);
    }
	/**
     * Same as prestartCoreThread except arranges that at least one
     * thread is started even if corePoolSize is 0.
     */
    void ensurePrestart() {
        int wc = workerCountOf(ctl.get());
        if (wc < corePoolSize)
            addWorker(null, true);
        else if (wc == 0)
            addWorker(null, false);
    }
	    /**
     * Starts all core threads, causing them to idly wait for work. This
     * overrides the default policy of starting core threads only when
     * new tasks are executed.
     *
     * @return the number of threads started
     */
    public int prestartAllCoreThreads() {
        int n = 0;
        while (addWorker(null, true))
            ++n;
        return n;
    }
	
	
}
-------------------------------------------------------------
为什么Mongodb的索引用了B树，而mysql用B+树:
B树和B+树的最重要的一个区别：B+树只有叶子节点存放数据，其余节点用来索引。而B树每个索引节点都会有Data域。这就决定了B+树更适合用来存储外部数据，即磁盘数据。
从InnoDB的角度来看，B+树用来充当索引，一般来说索引非常大，尤其是关系型数据库索引能达到亿级，所以为了减少内存的占用，索引也会被存储在磁盘上。
MySQL如何衡量查询效率：磁盘IO次数。B树的特点就是每层节点数目非常多，层数很少，目的就是减少磁盘IO次数，当查询数据的时候，最好的情况就是很快找到目标索引，然后读取数据，使用B+树可以很好完成这个目的。但是B树的每个节点都有Data域，无疑增大了节点大小，即增加了磁盘IO次数(磁盘IO一次读出的数据量大小是固定的，单个数据变大，每次读出的就少，IO次数增多)，而B+树除了叶子节点其他节点都不存储数据，节点小，磁盘IO次数就少。这是优点之一。
另一个优点：B+树所有的Data域在叶子节点，一般会进行一个优化，将所有的叶子节点用指针串起来，这样遍历叶子节点就能获得全部数据，就能进行区间访问了。
数据库索引采用B+树的主要原因是B树在提高磁盘IO性能的同时，并没有解决元素遍历的效率低下问题。正是为了解决这个问题，B+树应运而生。B+树只要遍历叶子节点就可以实现整个树的遍历。而且在数据库中基于范围的查询是非常频繁的，而B树不支持这样的操作(或者说效率太底)。
至于MongoDB为什么使用B树而不是B+树，从它的设计角度来考虑，它不是传统的关系型数据库，而是以Json格式存储的NOSQL,目的是高性能、高可用、易扩展。首先它摆脱了关系模型，上面所说的优点二需求就没那么强烈了，其次MySQL由于使用B树，数据都在叶子节点上，每次查询都需要访问叶子节点，而MongoDB使用B树，所有节点都有Data域，只要找到指定索引就可以进行访问，无疑单次查询平均快于MySQL。
总体来说，MySQL选择B+树，MongoDB选择B树，都是以自己的设计需求来选择。
B树的两个明显特点：
	1. 树内的每个节点都存储数据；
	2. 叶子节点之间无指针相邻；
B+树的两个明显特点：
	1. 数据只出现在叶子节点；
	2. 所有叶子节点增加了一个链接指针。

针对上面的B+树和B树的特点，我们做一个总结
(1)B树的树内存储数据，因此查询单条数据的时候，B树的查询效率不固定，最好的情况是O(1)。我们可以认为在做单一数据查询的时候，使用B树平均性能更好。但是，由于B树中各节点之间没有指针相邻，因此B树不适合做一些数据遍历操作。

(2)B+树的数据只出现在叶子节点上，因此在查询单条数据的时候，查询速度非常稳定。因此，在做单一数据的查询上，其平均性能并不如B树。但是，B+树的叶子节点上有指针进行相连，因此在做数据遍历的时候，只需要对叶子节点进行遍历即可，这个特性使得B+树非常适合做范围查询。

因此，我们可以做一个推论:没准是Mysql中数据遍历操作比较多，所以用B+树作为索引结构。而Mongodb是做单一查询比较多，数据遍历操作比较少，所以用B树作为索引结构。

那么为什么Mysql做数据遍历操作多？而Mongodb做数据遍历操作少呢？
因为Mysql是关系型数据库，而Mongodb是非关系型数据。
-------------------------------------------------------------
Redis的数据结构：
	1. String;
	2. Hash;
	3. List;
	4. Set;
	5. SortedSet;
	6. HyperLogLog;
	7. GEO;
另外Redission扩展了很多功能：
A: SpringCache, TomcatSession, HibernateCache
B: SpringSession, BloomFilter, SocredSortedSet, SortedSet
C: Queue, BlockingQueue, LuaScript, BlockingDeque, Deque
D: MapCache, Map, BitSet, SetMultimap, SetCache, Set, List, ListMultimap
E: Lock, ReadWriteLock, Semaphore, CountDownLatch, Geo
F: LiveObjectService, ExecutorSerivce, SchedulerService, RemoteService, AtomicLong, Publish/Subscribe

Redisson是一个在Redis的基础上实现的Java驻内存数据网格（In-Memory Data Grid）。它不仅提供了一系列的分布式的Java常用对象，还提供了许多分布式服务。其中包括(BitSet, Set, Multimap, SortedSet, Map, List, Queue, BlockingQueue, Deque, BlockingDeque, Semaphore, Lock, AtomicLong, CountDownLatch, Publish/Subscribe, Bloom filter, Remote service, Spring cache, Executor service, Live Object service, Scheduler service) Redisson提供了使用Redis的最简单和最便捷的方法。Redisson的宗旨是促进使用者对Redis的关注分离（Separation of Concern），从而让使用者能够将精力更集中地放在处理业务逻辑上。
Redisson底层采用的是Netty框架，支持Redis2.8以上版本，支持JDK1.6以上版本。

InnoDB目前有四种行格式: compact(简洁的), redundant(冗余的), dynamic(动态的), compress(压缩的)；

HTTP1.0和1.1的区别：
	1. 缓存处理：在Http1.0中主要使用header里的If-Modified-Since, Expires来做缓存判断的标准，HTTP1.1则引入了更多的缓存控制策略。例如：Entity tag, If-Unmodified-Since, If-Match, If-None-Match等更多可供选择的缓存头来控制缓存策略。
	2. 带宽优化及网络连接的使用：Http1.0中，存在一些浪费带宽的现象，比如客户端只是需要某个对象的一部分，而服务器却将整个对象传输过来，不支持断点续传功能。Http1.1则在请求头引入了range头域，它允许只请求资源的某部分，即返回码是206(Partial Content)，方便开发者自由选择以便于充分利用带宽和连接。
	3. 错误通知的管理：在Http1.1中新增了24个状态响应命，如409(Conflict)表示请求的资源与资源当前状态冲突；410(Gone)表示服务器上的某个资源被永久性的删除。
	4. Host头处理：在Http1.0中认为每台服务器都绑定一个唯一的IP地址，因此，请求消息中的URL并没有传递主机名(hostname)。但随着虚拟主机技术的发展，在一台物理服务器上可以存在多个虚拟主机（Multi-homed Web Servers），并且它们共享一个IP地址。HTTP1.1的请求消息和响应消息都应支持Host头域，且请求消息中如果没有Host头域会报告一个错误（400 Bad Request）
	5. 长连接和流水线：HTTP1.1支持长连接(PersistentConnection)和流水线(Pipelining)处理，在一个TCP连接上可以传送多个HTTP请求和响应，减少了建立和关闭连接的消耗和延迟。在HTTP1.1中默认开启:keep-alive，弥补了Http1.0每次请求都创建连接的缺点。请求的流水线（Pipelining）处理，在一个TCP连接上可以传送多个HTTP请求和响应，减少了建立和关闭连接的消耗和延迟。例如：一个包含有许多图像的网页文件的多个请求和应答可以在一个连接中传输，但每个单独的网页文件的请求和应答仍然需要使用各自的连接。  HTTP 1.1还允许客户端不用等待上一次请求结果返回，就可以发出下一次请求，但服务器端必须按照接收到客户端请求的先后顺序依次回送响应结果，以保证客户端能够区分出每次请求的响应内容。

聚簇索引的优势：
	看上去聚簇索引的效率明显低于非聚簇索引，因为每次使用辅助索引检索都要经过两次B+树查找，多此一举。
	1. 由于行数据和叶子节点存储在一起，同一页中会有多行数据，访问同一数据页不同行记录时，已经把页加载到了Buffer中，再次访问的时候，会在内存中完成，不必访问磁盘。这样主键和行数据是一起载入内存的，找到叶子节点就可以立刻将行数据返回了，如果按照主键ID来组织数据，获得数据更快。
	2. 辅助索引使用主键作为指针，而不是使用地址值作为指针的好处是：减少了当出现行移动或者数据页分裂时辅助索引的维护工作，使用主键值当作指针会让辅助索引占用更多的空间，换来的好处是InnoDB在移动行时无须更新辅助索引中的这个指针。也就是说行的位置(实现中通过16K的Page来定位)会随着数据库里数据的修改而发生改变(B+树节点分裂以及Page的分裂)，使用聚簇索引就可以保证不管这个主键B+树的节点如何变化，辅助索引树都不受影响。
	3. 聚簇索引适合用于在排序的场景，非聚簇索引不适合。
	4. 取出一定范围的时候，使用聚簇索引。
	5. 二级索引需要两次索引查找，而不是一次才能取到数据，因此存储引擎第一次需要通过二级索引找到索引的叶子节点，从而找到数据的主键，然后在聚簇索引中用主键再次查找索引，再找到数据。
	6. 可以把相关数据保存在一起。只需从磁盘读取少数的数据页就能获取相关数据。
聚簇索引的劣势：
	1. 维护索引很昂贵，特别是插入新行或者主键被更新导致分页(page split)的时候。建议在大量插入新行后，选择负载低的时间段，通过optimize table优化表，因为被移动的行数据可能造成碎片。使用独享表空间可以弱化碎片。
	2. 表因为使用UUID作为主键，使数据存储稀疏，这就会出现聚簇索引可能比全表扫描更慢。新插入的记录会插入到之前的记录中间，导致需要强制移动之前的记录。被写满且已经刷到磁盘上的页可能会被重新读取。建议使用int的auto_increment作为主键。主键的值是顺序的，所以 InnoDB 把每一条记录都存储在上一条记录的后面。当达到页的最大填充因子时（InnoDB 默认的最大填充因子是页大小的 15/16，留出部分空间用于以后修改），下一条记录就会写入新的页中。一旦数据按照这种顺序的方式加载，主键页就会近似于被顺序的记录填满（二级索引页可能是不一样的）。
	3. 如果主键比较大的话，那辅助索引将会变更大，因为辅助索引的叶子存储的是主键值；过长的主键值，会导致聚簇索引的非叶子节点占用更多的物理空间。

使用Http长连接之后，客户端、服务端怎么知道本次传输结束呢？两部分：1是判断传输数据是否达到了Content-Length指示的大小；2动态生成的文件没有Content-Length，它是分块传输（chunked），这时候就要根据chunked编码来判断，chunked编码的数据在最后有一个空chunked块，表明本次传输数据结束。

TCP的keep alive是检查当前TCP连接是否活着；HTTP的Keep-alive是要让一个TCP连接活久点。它们是不同层次的概念。
TCP keep alive的表现：当一个连接“一段时间”没有数据通讯时，一方会发出一个心跳包（Keep Alive包），如果对方有回包则表明当前连接有效，继续监控。这个“一段时间”可以设置。

HTTP的长连接和短连接本质上是TCP长连接和短连接。HTTP属于应用层协议，在传输层使用TCP协议，在网络层使用IP协议。IP协议主要解决网络路由和寻址问题，TCP协议主要解决如何在IP层之上可靠地传递数据包，使得网络上接收端收到发送端所发出的所有包，并且顺序与发送顺序一致。TCP协议是可靠的、面向连接的。

TCP三次握手的流程和状态流转：
	1. 客户端发送SYN报文，并设置发送序号为X(SYN=1, Ack=0, seq=X)；之后客户端状态变为SYN-SEND；
	2. 服务端收到之后，回复SYN+ACK报文，并置发送序号为Y(SYN=1, ACK=1, seq=Y, ack=X+1)；之后服务端状态为SYN-RECEIVED;
	3. 客户端收到之后，回复ACK报文(ACK=1,Seq=X+1, acK=Y+1)；客户端状态变为ESTABLISHED,服务端收到之后也变成ESTABLISHED。
TCP四次挥手的流程和状态流转：
	1. 主动发起方发起FIN=1,seq=u报文，发送之后主动发起方状态变为FIN-WAIT-1;
	2. 被动方接收到之后，回复ACK=1, seq=v,ack=u+1报文，之后被动方状态变为CLOSE-WAIT;
		主动发起方收到回复后，状态变为FIN-WAIT-2；之后等待被动方发起FIN报文；
	3. 被动方在发送完最后的报文后，发起FIN=1, ACK=1, seq=w, ack=u+1的报文，之后被动方状态变为LAST-ACK；
	4. 主动发起方收到之后，回复ACK=1,seq=u+1, ack=w+1，之后主动发起方状态变成TIME-WAIT。
		被动方收到回复后，状态从LAST-ACT变成CLOSED；
		主动方在变成TIME-WAIT状态后的2MSL时间之后，状态变成CLOSED。

TCP中的6个标志位：URG, ACK, PSH, RST, SYN, FIN；
在这6个标志中，TCP的三次握手和四次挥手用到了3个：
SYN：SYN=1表示这是一个连接请求或连接接收报文。在建立连接时用来同步序号(在建立连接的时候，提醒对方记录本方的起始序号)。当SYN=1而ACK=0时，表明这是一个连接请求报文段。对方若是同意建立连接，则响应的报文段中使SYN=1,ACK=1。因此SYN=1表示该报文是一个连接请求或者一个连接请求接收报文。
ACK：确认号只有在该位设置为1的时候才生效，当该位为0表示确认号无效。TCP规定，在TCP连接建立后所有传送的报文段ACK都必须设置为1.
FIN:当FIN=1时，表明该报文段的发送方的数据已经发送完毕，并要求释放连接。

此外，还需要序号和确认号：
	序号：占4个字节，它的范围在0-2^32-1，序号随着通信的进行不断的递增，当达到最大值的时候重新回到0在开始递增。TCP是面向字节流的，在一个TCP连接中传送的字节流中的每一个字节都按照顺序编号。整个要传送的字节流的起始号必须在连接建立时设置。首部中的序列号字段指的是本报文段所发送的数据的第一个字节的序号。例如，一个报文序号是301，而携带的数据共有100字节。则表示本次报文中的序号是301，下一个报文的序号是401.重复一下，每一个报文的序号是该报文包含的字节中第一个字节的编号。
	确认号：占4个字节，确认号，是对下一个想要接受的字节的期望，这里隐式确认了对上一个数据包的成功接收。如上例，在成功接收了序号为301的数据包，想要接收下一个数据包因为上个数据包包含100字节，所以此时的确认号应该是401，表示希望接收下一个序号是401的数据包。
-----------------------------------------
为什么需要进行三次握手：
为什么客户端还要发送确认呢？这主要是为了防止已失效的连接请求报文突然又传送到了服务端，导致错误。
所谓“已失效的连接请求报文段”是这样产生的。考虑一种正常情况。客户端发出连接请求，但因连接请求丢失而未收到确认。于是客户端再次重传一次连接请求。后来收到了确认建立了连接。数据传输完毕后，就释放了连接。客户端共发送了两个连接请求的报文段，其中第一个丢失，第二个到达了服务端。没有“已失效的连接请求报文段”。
假定出现一种异常情况，即客户端发出的第一个连接请求报文段并没有丢失，而是在某些网络节点长时间滞留了，以致延误到连接释放以后的某个时间才到服务端。本来这是一个已失效的报文段。但是服务端收到此失效的连接请求报文段后，就误认为是客户端有发出一次新的连接请求。于是就向客户端发出确认报文段，同意建立连接。假定不采用三次握手，那么只要服务端发出确认，新的连接就建立了。

由于现在客户端并没有发出建立连接的请求，因此不会理睬服务端的确认，也不会向服务端发送数据。但服务端却以为新的连接已经建立了，并一直等待客户端发来数据。服务端的许多资源就这样拜拜浪费了。

采用三次握手的办法可以防止上述现象的发生。例如在刚才的情况下，客户端不会向服务端的确认发出确认。服务端由于收不到确认，就知道客户端并没有要求建立连接。

另一种解释：
这个问题的本质是, 信道不可靠, 但是通信双发需要就某个问题达成一致. 而要解决这个问题, 无论你在消息中包含什么信息, 三次通信是理论上的最小值. 所以三次握手不是TCP本身的要求, 而是为了满足"在不可靠信道上可靠地传输信息"这一需求所导致的. 请注意这里的本质需求,信道不可靠, 数据传输要可靠. 三次达到了, 那后面你想接着握手也好, 发数据也好, 跟进行可靠信息传输的需求就没关系了. 因此,如果信道是可靠的, 即无论什么时候发出消息, 对方一定能收到, 或者你不关心是否要保证对方收到你的消息, 那就能像UDP那样直接发送消息就可以了”。这可视为对“三次握手”目的的另一种解答思路。
-----------------------------------------
为什么TIME-WAIT状态需要等2*MSL之后才能返回CLOSED状态：
	因为虽然双方都同意关闭连接了，而且挥手的4次报文都发送完毕，按理可以直接CLOSED状态，但是网络的不可靠性，无法保证最后发送的ACK报文一定被被动方收到，就是说对方处于LAST-ACK状态下的Socket可能会因为超时未收到ACK报文，而重发FIN报文，所以这个TIME-WAIT状态的作用是用来重发可能丢失的ACK报文。

TCP有哪些状态：共11种：
1. CLOSED：初始状态，表示TCP连接是“关闭着的”或“未打开的”。
2. LISTEN ：表示服务器端的某个SOCKET处于监听状态，可以接受客户端的连接。
3. SYN_RCVD ：表示服务器接收到了来自客户端请求连接的SYN报文。在正常情况下，这个状态是服务器端的SOCKET在建立TCP连接时的三次握手会话过程中的一个中间状态，很短暂，基本上用netstat很难看到这种状态，除非故意写一个监测程序，将三次TCP握手过程中最后一个ACK报文不予发送。当TCP连接处于此状态时，再收到客户端的ACK报文，它就会进入到ESTABLISHED 状态。
4. SYN_SENT ：这个状态与SYN_RCVD 状态相呼应，当客户端SOCKET执行connect()进行连接时，它首先发送SYN报文，然后随即进入到SYN_SENT 状态，并等待服务端的发送三次握手中的第2个报文。SYN_SENT 状态表示客户端已发送SYN报文。
5. ESTABLISHED ：表示TCP连接已经成功建立。
6. FIN_WAIT_1 ：其实FIN_WAIT_1 和FIN_WAIT_2 两种状态的真正含义都是表示等待对方的FIN报文。而这两种状态的区别是：FIN_WAIT_1状态实际上是当SOCKET在ESTABLISHED状态时，它想主动关闭连接，向对方发送了FIN报文，此时该SOCKET进入到FIN_WAIT_1 状态。而当对方回应ACK报文后，则进入到FIN_WAIT_2 状态。当然在实际的正常情况下，无论对方处于任何种情况下，都应该马上回应ACK报文，所以FIN_WAIT_1 状态一般是比较难见到的，而FIN_WAIT_2 状态有时仍可以用netstat看到。
7. FIN_WAIT_2 ：上面已经解释了这种状态的由来，实际上FIN_WAIT_2状态下的SOCKET表示半连接，即有一方调用close()主动要求关闭连接。注意：FIN_WAIT_2 是没有超时的（不像TIME_WAIT 状态），这种状态下如果对方不关闭（不配合完成4次挥手过程），那这个 FIN_WAIT_2 状态将一直保持到系统重启，越来越多的FIN_WAIT_2 状态会导致内核crash。
8. TIME_WAIT ：表示收到了对方的FIN报文，并发送出了ACK报文。 TIME_WAIT状态下的TCP连接会等待2*MSL（Max Segment Lifetime，最大分段生存期，指一个TCP报文在Internet上的最长生存时间。每个具体的TCP协议实现都必须选择一个确定的MSL值，RFC 1122建议是2分钟，但BSD传统实现采用了30秒，Linux可以cat /proc/sys/net/ipv4/tcp_fin_timeout看到本机的这个值），然后即可回到CLOSED 可用状态了。如果FIN_WAIT_1状态下，收到了对方同时带FIN标志和ACK标志的报文时，可以直接进入到TIME_WAIT状态，而无须经过FIN_WAIT_2状态。（这种情况应该就是四次挥手变成三次挥手的那种情况）
9. CLOSE_WAIT ：表示正在等待关闭。怎么理解呢？当对方close()一个SOCKET后发送FIN报文给自己，你的系统将会回应一个ACK报文给对方，此时TCP连接则进入到CLOSE_WAIT状态。接下来呢，需要检查自己是否还有数据要发送给对方，如果没有的话，那你也就可以close()这个SOCKET并发送FIN报文给对方，即关闭自己到对方这个方向的连接。有数据的话则看程序的策略，继续发送或丢弃。简单地说，当你处于CLOSE_WAIT 状态下，需要完成的事情是等待你去关闭连接。
10. LAST_ACK ：当被动关闭的一方在发送FIN报文后，等待对方的ACK报文的时候，就处于LAST_ACK 状态。当收到对方的ACK报文后，也就进入到CLOSED状态。
11. CLOSING:这种状态实际情况很少见。正常情况下，主动方发出FIN报文后，应该先收到对方的ACK报文，再收到被动方的FIN报文。但是CLOSING状态表示一方发送FIN报文后，并没有收到ACK报文，反而收到了FIN报文。什么情况下会出现这种情况呢？当双方同时close()时，就出现同时发送FIN报文，就会出现CLOSING状态。
UncaughtExceptionHandler

BIO:同步阻塞IO，服务器实现模式为一个连接一个线程，即客户端有连接请求时就需要启动一个线程进行处理，如果这个连接不做任何事情会造成不必要的线程开销，当然可以通过线程池机制改善。
NIO:同步非阻塞IO，服务器实现模式为一个请求一个线程，即客户端发送的连接都会注册到多路复用器上，多路复用器轮询到连接有I/O请求时才启动一个线程进行处理。
AIO(NIO2.0):异步非阻塞IO，服务器实现模式为一个有效请求一个线程，客户端的I/O请求都是由OS先完成再通知应用去启动线程进行处理。

NIO和BIO的区别：
	1. NIO以缓冲的方式处理数据，BIO是以最基础的字节流形式写入和读出的。在效率上，NIO效率比BIO效率会高。
	2. NIO不再是以BIO的方式用OutputStream和InputStream输入输出流来处理数据，而是采用了通道和缓冲区的形式处理数据。
	3. NIO的通道可以是双向的，但是IO中的流只能是单向的。
	4. NIO的缓冲区可以进行分片，可以建立只读缓冲区、直接缓冲区和间接缓冲区，直接缓冲区为加快IO速度以一种特殊的方式分配内存的缓冲区。
	5. NIO采用多路复用的IO模型，BIO采用阻塞IO。

NIO是一种同步非阻塞的IO模型，也是IO多路复用的基础，成为解决高并发与大量连接、IO处理问题的有效方式。

synchronized能防止指令重排序吗：能，java有序性根据happens-before原则，其中有一条解锁操作happen-before加锁操作。
单例模式的double check单例是否需要对实例使用volatile修饰？需要。
实际上由于第一个if没有synchronized，所以synchronized带来的有序性对第一个if是不生效的。于是就会出现一种情况：就是new Singleton()时，先将引用分配给了instance，后创建实例。这样在其他线程进入到第一个if的时候就会为false，使用未生成实例的引用。这个时候使用volatile实际上是保证了第一个if读的时候的有序性，对volatile变量的写happen-before读，从而禁止了new Singleton()时的重排序。
----------------------------------------------------------
volatile是如何实现可见性的：
用volatile修饰的共享变量进行写操作的时候会多出Lock前缀的指令。
Lock前缀的指令主要有两方面的影响：
	1. 将当前处理器缓存行的数据写回内存；
	2. 这个写回内存的操作回使得其他CPU里缓冲了该内存地址的数据无效。

为了提高处理速度，处理器不直接和内存进行通信，而是先将内存的数据读到内部缓冲(L1,L2等)后再进行操作，但操作完不知道何时会写到内存。如果对声明了volatile的变量进行写操作，JVM就会向处理器发送一条Lock前缀的指令，将这个变量所在缓存行的数据写回内存。但是，就算写回内存，如果其他处理器缓冲的值还是旧的，再只需计算操作还是会有问题。
所以，在多处理器下，为了保证各个处理器的缓冲是一致的，就会实现缓存一致性协议，每个处理器通过嗅探在总线上传播的数据来检查自己缓存的值是不是过期了，当处理器发现自己缓存行对应的内存地址被修改，就会将当前处理器的缓存行设置为无效，当处理器对这个数据进行修改操作的时候，会重新从系统内存把数据读到处理器缓存里。
因此，可以得出如下结论：
	1. Lock前缀的指令会引起处理器缓存写回内存；
	2. 一个处理器的缓存回写到内存会导致其他处理器的缓存失效；
	3. 当处理器发现本地缓存失效后，就会从内存重读该变量数据，即可以获取当前最新值。
----------------------------------------------------------
volatile如何阻止重排序：
	为了失效volatile内存语义，编译器会在生成字节码时，在指令序列中插入内存屏障来禁止特定类型的处理器重排序。对于编译器来说，一个最优布置来最小化插入屏障是几乎不可能的，为此JMM采取了保守策略：
	1. 在每个volatile写操作前面插入一个StoreStore屏障
	2. 在每个volatile写操作后面插入一个StoreLoad屏障
	3. 在每个volatile读操作前面插入一个LoadLoad屏障
	4. 在每个volatile读操作后面插入一个LoadStore屏障

synchronized的特性：原子性，可见性，有序性，可重入性。
每一个锁都对应一个monitor对象，在HotSpot虚拟机中它是由ObjectMonitor实现的(C++实现)。每个对象都存在着一个monitor与之关联，对象与其monitor之间的关系有存在多种实现方式，如monitor可以与对象一起创建销毁或当线程试图获取对象锁时自动生成，但当一个monitor被某个线程持有后，它便处于锁定状态。
ObjectMonitor() {
	_header       = NULL;
	_count        = 0;   //锁计数器
	_waiters      = 0;
	_recursions   = 0;
	_object       = NULL;
	_owner        = NULL;
	_WaitSet      = NULL;   //处于wait状态的线程，会被加入到_WaitSet
	_WaitSetLock  = 0;
	_Responsilbe  = NULL;
	_succ         = NULL;
	_cxq          = NULL;
	FreeNext      = NULL;
	_EntryList    = NULL;   //处于等待锁block状态的线程，会被加入到该列表
	_SpinFreq     = 0;
	_SpinClock    = 0;
	OwnerIsThread = 0;
}

锁消除：消除锁是虚拟机一种锁优化，这种优化更彻底，在JIT编译时，对运行上下文进行扫描，去除不可能存在竞争的锁。比如下面代码的method1和method2的执行效率是一样的，因为object锁是私有变量，不存在锁竞争关系。
锁粗化：是虚拟机对另外一种极端情况的优化处理，通过扩大锁的范围，避免反复加锁和释放锁。
---------------------------------
B+索引和Hash索引的区别，优缺点
	1. 如果是等值查询，哈希索引有明显优势。当然前提是键值是唯一的，如果键值不唯一，出现哈希冲突就具体分析；
	2. 如果是范围查询检索，哈希索引无用武之地。因为原先有序的键值，经过哈希算法后，有可能变成不连续的了，就没办法利用索引完成范围查询检索；
	3. 哈希索引也没办法利用索引完成排序，以及like模糊查询；
	4. 哈希索引页不支持多列联合索引的最左匹配规则；
	5. B+树索引的关键字检索效率比较平均，在有大量重复键值情况下，哈希索引的效率是很低的，因此存在哈希碰撞问题。
	
---------------------------------
在MySQL中，只有Heap/Memory引擎才能显式支持哈希索引(NDB也支持)，InnoDB引擎的自适应哈希索引不在此列。
JDK7和JDK8中HashMap的区别：
	1. 在哈希碰撞时，JDK7使用头插入法，JDK8使用尾插入法。因为JDK7用单链表进行纵向延伸，当采用头插法时会容易出现逆序且链表死循环问题。JDK8因为加入了红黑树使用尾插入法，能够避免出现逆序且链表死循环问题。
	2. 扩容后数据存储位置计算方式不同。JDK7使用hash值和扩容后的容量&运算。JDK8使用hash&oldCap的值，如果0原来位置，否则原来位置+oldCap。
	3. hash值的计算方式不同：JDK7用了9次扰动处理=4次位运算+5次异或，而JDK8只用了2次扰动处理=1次位运算+1次异或。
	4. 扩容时机不同：JDK7先扩容再插入，JDK8先插入后扩容。
	5. JDK7使用数组+单链表的数据结构。JDK8是数组+链表+红黑树(当链表的深度达到8的时候，也就是默认阈值，就会自动扩容把链表转成红黑树的数据结构来把时间复杂度从O(n)变成O(logN)提高了效率）

)。
为什么HashMap中String、Integer这样的包装类适合作为key键:
	1. String, Integer等包装类的特性包装了hash值的不可变更性，保证计算准确性；
	2. 有效减少了发生hash碰撞的机率。
	
public class WeakHashMap<K, V> extends AbstractMap<K, V>
		implements Map<K, V> {
	...
	// Reference queue for cleared WeakEntires
	private final ReferenceQueu<Object> queue = new ReferenceQueue<>();
	...
	// The entries in this hash table extend WeakReference, using its main ref field as the key.
	private static class Entry<K,V> extends WeakReference<Object> implements Map.Entry<K,V> {
		V value;
		final int hash;
		Entry<K,V> next;
		
		Entry(Object key, V value, ReferenceQueue<Object> queue, int hash, Entry<K,V> next) {
			super(key, queue);
			this.value = value;
            this.hash  = hash;
            this.next  = next;
		}
	}
	
	//Returns the table after first expunging stale entries
	private Entry<K, V>[] getTables() {
		expungeStaleEntries();
		return table;
	}
	//Expunges stale entries from the table.
	private void expungeStaleEntries() {
		for (Object x;(x = queue.poll()) != null;) {
			synchronized(queue) {
				@SuppressWarnings("unchecked")
				Entry<K, V> e = (Entry<K, V>)x;
				int i = indexFor(e.hash, table.length);
				
				Entry<K, V> prev = table[i];
				Entry<K, V> p = prev;
				while (p != null) {
					Entry<K, V> next = p.next;
					if (p == e) {
						if (prev == e)
							table[i] = next;
						else 
							prev.next = next;
						// Must not null out e.next;
						// stale entries may be in use by HashIterator
						e.value = null;   //help GC
						size --;
						break;
					}
					prev = p;
					p = next;
				}
			}
		}
	}
}
JDK8中的WeakHashMap在发生哈希碰撞时，使用链表存储冲突元素，使用头插入法。		
HashMap和Weakhashmap的区别

如何安全发布对象：
	1. 在静态初始化函数中初始化一个对象引用；
	2. 将对象的引用保存到volatile类型域或者AtomicReference对象中;
	3. 将对象的引用保存到某个正确构造对象的final类型域中；
	4. 将对象的引用保存到一个由锁保护的域中。

Netty的零拷贝主要体现在三个方面：
	1. Netty的接收和发送ByteBuf采用Direct Buffer，使用堆外内存进行Socket读写，不需要进行字节缓冲区的二次拷贝。如果使用传统的堆内存(Heap buffer)进行Socket读写，JVM会将堆内存Buffer拷贝一份到直接内存，然后才写入socket。相比于堆外直接内存，消息在发送过程中多了一次缓冲区的内存拷贝。
	2. Netty提供了组合Buffer对象，可以聚合多个ByteBuff对象，用户可以像操作一个Buffer方便对组合Buffer进行操作，避免了传统通过内存拷贝的方式将几个小Buffer合并成一个大Buffer。
	3. Netty的文件传输采用了transferTo方法，它可以直接将文件缓冲区的数据发送到目标Channel，避免传统通过循环write方式导致内存拷贝问题。

Zero Copy模式中，避免了数据在用户空间和内核空间之间的拷贝，从而提高了系统的整体性能。Linux中的sendfile以及Java NIO中的FileChannel.transferTo()方法都实现了零拷贝的功能，而在Netty中也通过在FileRegion中包装NIO的FileChannel.transferTo()方法实现了零拷贝。

在HTTP/1.1协议中，浏览器客户端在同一时间，针对同一域名下的请求有一定数量限制，超过限制数目的请求会被阻塞。
而HTTP/2的多路复用(Multiplexing)，则允许同时通过单一的HTTP/2连接发起多重的请求-响应消息。
因此HTTP/2可以很容易的实现多流并行而不用依赖建立多个TCP连接，HTTP/2把HTTP协议通信的基本单位缩小为一个一个的帧，这些帧对应着逻辑流中的消息，并行地在同一个TCP连接上双向交换消息。
在不改动HTTP/1.X的语义、方法、状态码、URI以及首部字段的情况下，HTTP/2如何做到突破HTTP1.1的性能限制，改进传输性能，实现低延迟和高吞吐量的？关键之一在应用层(HTTP/2)和传输层(TCP/UDP)之间增加了一个二进制分帧层。

多路复用：直白的说就是所有的请求都是通过一个 TCP 连接并发完成。HTTP/1.x 虽然通过 pipeline 也能并发请求，但是多个请求之间的响应会被阻塞的，所以 pipeline 至今也没有被普及应用，而 HTTP/2 做到了真正的并发请求。同时，流还支持优先级和流量控制。当流并发时，就会涉及到流的优先级和依赖。优先级高的流会被优先发送。图片请求的优先级要低于 CSS 和 SCRIPT，这个设计可以确保重要的东西可以被优先加载完。
Server Push：服务端能够更快的把资源推送给客户端。例如服务端可以主动把 JS 和 CSS 文件推送给客户端，而不需要客户端解析 HTML 再发送这些请求。当客户端需要的时候，它已经在客户端了。

索引下推优化是MySQL5.6版本中新加的功能。
索引条件下推优化：Index Condition Pushdown Optimization是MySQL使用索引从表中检索行的情况的优化。如果没有ICP，存储引擎会遍历索引以查找表中的行，并将它们返回给MySQL服务器，由server层再做一波筛选。启用ICP后，如果只使用索引中的列来评估where条件的某些部分，MySQL服务器会将where条件推送到存储引擎。然后，存储引擎使用索引条目评估推送的索引条件，并且仅当满足该条件时才从表中读取行。
ICP可以减少存储引擎必须访问基表的次数以及MySQL服务器必须访问存储引擎的次数。
ICP的适用性受以下条件限制：
	1. 当需要访问全表行时，ICP用于range, ref, eq_ref和ref_or_null访问方法。
	2. ICP可用于InnoDB和MyISM表。(MySQL5.6中的分区表不支持ICP；MySQL5.7解决了这个问题)
	3. 对于InnoDB表，ICP仅用于二级索引。ICP的目标是减少全行读取的数量，从而减少IO操作。对于InnoDB聚簇索引，完整记录已经读入InnoDB缓冲区，这种情况下使用ICP，不会降低IO。
	4. 引用子查询的条件无法下推。
	5. 无法下推涉及存储函数的条件。存储引擎无法调用存储的函数。
	6. 触发条件无法下推。
默认情况下，启用索引条件下推。可以用optimizer_switch系统变量设置index_condition_pushdown设置
set optimizer_swith = "index_condition_pushdown=on/off";
EXPLAIN时，使用ICP的查询会在Extra列出现 Using index condition关键字

MySQL5.6对索引做了哪些优化:
	1. MRR: Mulit-Range Read Optimization;
	2. ICP: Index Condition Pushdown Optimization(索引下推)
	3. 覆盖索引
	4. BKA(Batched Key Access)
-----------------------
MRR是优化器将随机IO转化为顺序IO，以降低IO开销的手段。
二级索引中存储的是索引列和主键值，当查询列不都存在与索引列中时（即不是覆盖索引的情况），需要回表操作。然而回表获取完整用户记录可能回产生随机IO（当数据量较多且比较分散时，随机IO性能较低），为减少这种随机IO，MySQL首先只在二级索引中查询，统计关联行的主键，然后根据主键值排序， 最后从聚簇索引中按照主键排序获取完整记录。

使用 MRR 时的查询过程：
	1. 优化器将从二级索引中查询到的记录放到缓冲区中；
	2. 如果缓冲区已满，或者扫描到二级索引文件末尾，使用快排对缓冲区的数据按照主键排序；
	3. 用户线程根据缓冲区的内容，从聚簇索引中获取数据；
	3. 当缓冲区内容读取完毕时，重复上述过程，直到扫描结束
如何查看一条SQL是不是MRR呢？我们EXPLAIN执行一条SQL时，使用MRR的查询会在Extra列出现Using MRR关键字。
用optimizer_switch变量控制是否开启MRR，默认是开启的，关闭的话set optimizer_switch = 'mrr=off'
MRR优化的几个好处：
	1. 使数据访问由随机变为顺序访问，查询辅助索引时，首先把查询结果按照主键进行排序，按照主键顺序进行回表查找；
	2. 减少缓冲池中页被替换的次数；
	3. 批量处理对键值的操作。
-----------------------
where条件可以分为三大类：Index Key(First Key, Last Key)，Index Filter以及Table Filter。
----------------------------------
HTTP/2的特点/优势：
	1. 多路复用的单一长连接
		1.1 单一长连接
			在HTTP/2中，客户端向某个域名的服务器请求页面的过程中，只会创建一条TCP连接，即使这页面包含上百个资源。单一的连接应该是HTTP/2的主要优势，单一的连接能减少TCP握手带来的时延。HTTP/2中用单一的长连接，避免了创建多个TCP连接带来的网络开销，提高了吞吐量。
		1.2 多路复用 
			HTTP/2虽然只有一条TCP连接，但是在逻辑上分成了很多stream。
			HTTP/2把要传输的信息分割成一个个二进制帧，首部信息会被封装到Header Frame，相应的request body就放在Data Frame，一个帧把不同的http请求或响应区分开来了。但是，这里要求同一个请求或者响应必须是有序的，要保证FIFO，但是不同的请求或响应帧可以互相穿插。这就是HTTP/2的多路复用，充分利用了网络带宽，提高了并发度。
	2. 头部压缩和二进制格式
		HTTP/1.x一直都是plain text,便于阅读和调试。但是对于HTTPS把plain text变成了二进制，这个优点没有了。
		HTTP/2使用HPACK压缩来压缩头部，减少报文大小。
	3. 服务端推送Server Push 
		主要思想是：当一个客户端请求资源X，而服务器知道它很可能需要资源Z的情况下，服务器可以在客户端发送请求前，主动将资源Z推送给客户端。
----------------------------------
HTTP2， 为解决以上问题而生。
	1. 允许多个request/response在同一个tcp链接上发送
	2. 高效压缩头部（http header）
	3. 二进制协议，真正的多路复用
	4. 还有自己的流量控制，保证各个 stream不被互相干扰；
	5. 支持请求分优先级发送，优先级越高如核心 css、html，优先发给客户端
	6. 支持服务器预测并推送客户端可能需要的资源，让客户端先做缓存（server push），榨干服务器
	7. 兼容 HTTP1.1 的语义，尽可能一致。
-----------------------------------
HTTP/2的Stream有流量控制功能，HTTP/2的接收方通过WINDOW_UPDATE帧告诉对方字节准备接收多少字节的数量，注意只有DATA帧才会受限制，因为其他帧都不大，而且也比较重要。

Netty高性能的原因：
	1. 基于IO多路复用模型；
	2. 零拷贝；
	3. 基于NIO的Buffer;
	4. 基于内存池的缓冲区重用机制；
	5. 无锁化的串行设计理念；
	6. IO操作的异步处理；
	7. 提供对protobuf等高性能序列化协议支持；
	8. 支持对TCP进行更加灵活的配置。

Netty中的零拷贝与操作系统层面上的零拷贝不完全一样，Netty的零拷贝完全是在用户态(Java层面)的，更多的是数据操作的优化。Netty的零拷贝主要体现在5方面：
	1. Netty的接收和发送ByteBuf使用直接内存进行Socket读写，不需要进行字节缓冲区的二次拷贝。如果使用JVM的堆内存进行Socket读写，JVM会将堆内存Buffer拷贝一份到直接内存中，然后才写入Socket中。相比于使用直接内存，消息在发送过程中多了一次缓冲区的内存拷贝。
	2. Netty的文件传输调用FileRegion包装的transferTo方法，可以直接将文件缓冲区的数据发送到目标Channel，避免通过循环while方式导致的内存拷贝问题。
	3. Netty提供CompositeByteBuff类，可以将多个ByteBuf合并为一个逻辑上的ByteBuf，避免了各个ByteBuf之间的拷贝。
	4. 通过wrap操作，可以将byte[]数组、ByteBuf、ByteBuffer等包装成一个Netty ByteBuf对象，进而避免拷贝操作。
	5. ByteBuf支持slice操作，可以将ByteBuf分解为多个分享同一个存储区域的ByteBuf，避免内存的拷贝。
----------------------------------
Batched Key Access(BKA)和Block Netsted-Loop(BNL)
Batched Key Access(BKA)提高表join性能的算法。当被join的表能够使用索引时，就先排好序，然后再去检索被join的表，和MRR类似，实际上MRR也可以想象成二级索引和primary key的join，如果被join的表上没有索引，则使用老版本的的BNL策略(Block Netsted-Loop)。

BKA原理
	对于多表join语句，当MySQL使用索引访问第二个join表的时候，使用一个join buffer来收集第一操作对象生成的相关列值。BKA构建好key后，批量传给引擎层做索引查找。key是通过MRR接口提交给引擎的(MRR目的是为了顺序)MRR使得查询更有效率。
	大致过程如下：
		1. BKA使用join buffer保存由join的第一个操作产生的符合条件的数据；
		2. 然后BKA算法构建key来访问被连接的表，并批量使用MRR接口提交keys到数据库存储引擎去查找；
		3. 提交keys之后，MRR使用最佳的方式来获取行并反馈给BKA。
BNL和BKA都是批量的提交一部分行给被join的表，从而减少访问的次数，它们之间的区别如下：
	1. BNL比BKA出现的早，BKA直到5.6才出现，而BNL在5.1就存在；
	2. BNL主要用于当被join的表上无索引；
	3. BKA主要是指被join表上有索引可以利用，那么就在行提交给被join的表之前，对这些行按照索引字段进行排序，因此减少了随机IO，排序才是两者最大的区别，但是如果被join的表没有索引，就使用BNL。
Using join buffer(Batched Key Access)和Using join buffer(Block Nested Loop)
相关参数：
	BAK使用了MRR,要想使用BAK必须打开MRR功能，而MRR基于mrr_cost_based的成本估算并不能包装总是使用MRR，官方推荐设置mrr_cost_based=off来总是开启MRR功能。打开BAK功能：
	set optimizer_switch='mrr=on,mrr_cost_based=off,batched_key_access=on';
	BKA使用join buffer size来确定buffer的大小，buffer越大，访问被join的表/内部表就越顺序。
	BNL默认是开启的，设置BNL相关参数：
		set optimizer_switch='block_nested_loop';
支持inner join, outer join, semi-join operations, including nested outer joins.
BKA主要适用于join的表上有索引可利用，无索引只能使用BNL。
----------------------------------
Nested Loop Join算法：将驱动表/外部表的结果集作为循环基础数据，然后循环该结果集，每次获取一条数据作为下一个表的过滤条件查询数据，然后合并结果，获取结果集返回给客户端。Nested-Loop一次只将一行传入内层循环，所以外层循环(的结果集)有多少行，内部循环便要执行多少次，效率非常差。
Block Nested-Loop Join算法：将外层循环的行/结果集存入join buffer，内存循环的每一行与整个buffer中的记录做比较，从而减少内存循环的次数，主要用于当被join的表上无索引。
Batched Key Access算法：当被join的表能够使用索引时，就先排序，然后再去检索被join的表。对这些行按照索引字段进行排序，因此减少了随机IO。如果被join的表上没有索引，则使用老版本的BNL策略(Block Nested-Loop)。

Java提供4种引用类型的目的：
	1. 可以通过代码的方式决定某些对象的生命周期；
	2. 有利于JVM进行垃圾回收。

强引用。特点：我们平常典型编码Object obj = new Object()中的obj就是强引用。通过关键字new创建的对象所关联的引用就是强引用。 当JVM内存空间不足，JVM宁愿抛出OutOfMemoryError运行时错误（OOM），使程序异常终止，也不会靠随意回收具有强引用的“存活”对象来解决内存不足的问题。对于一个普通的对象，如果没有其他的引用关系，只要超过了引用的作用域或者显式地将相应（强）引用赋值为 null，就是可以被垃圾收集的了，具体回收时机还是要看垃圾收集策略。

软引用：通过SoftReference类实现。软引用的生命周期比强引用短一些。只有当JVM认为内存不足时，才会试图回收软引用指向的对象：即JVM会确保在抛出OOM之前，清理软引用指向的对象。软引用可以和一个引用队列ReferenceQueue联合使用，如果软引用所引用的对象被垃圾回收器回收，JVM就会把这个软引用加入到与之关联的引用队列中。后续，可以调用ReferenceQueue的poll()方法来检查是否有所关心的对象被回收。如果队列为空，将返回一个null，否则该方法返回队列中前面的一个Reference对象。
软引用的应用场景：软引用通常用来实现内存敏感的缓存。如果还有空闲内存，就可以暂时保留缓存，当内存不足时清理掉，这样就保证了使用缓存的同时，不会耗尽内存。

弱引用：弱引用通过WeakReference类实现，弱引用的生命周期比软引用短。在垃圾回收器线程扫描它所管辖的内存区域的过程中，一旦发现了具有弱引用的对象，不管当前内存空间足够与否，都会回收它的内存。由于垃圾回收器是一个优先级很低的线程，因此不一定会很快回收弱引用的对象。弱引用可以和一个引用队列ReferenceQueue联合使用，如果弱引用所引用的对象被垃圾回收，JVM就会把这个弱引用加入到与之关联的引用队列中。
弱引用应用场景：弱引用同样可用于内存敏感的缓存。

虚引用的应用场景：可用来跟踪对象被垃圾回收器回收的活动，当一个虚引用关联的对象被垃圾收集器回收之前会收到一条系统通知。

虚引用和软引用/弱引用不同，它并不影响对象的生命周期。用PhantomReference类表示。如果一个对象与虚引用关联，则跟没有引用与之关联一样，在任何时候都可能被垃圾回收器回收。虚引用主要用来跟踪对象被垃圾回收的活动。
虚引用必须和引用队列关联使用，当垃圾回收器准备回收一个对象时，如果发现它还有虚引用，就会把这个虚引用加入到与之关联的引用队列中。程序可以通过判断引用队列中是否已经加入了虚引用，来了解被引用的对象是否将要被垃圾回收。如果程序发现某个虚引用已经被加入到引用队列，那么就可以在所引用的对象的内存被回收之前采取必要的行动。

concurrent包用过哪些类：
java.uti.concurrent包下：
AbstractExecutorService, ArrayBlockingQueue,BlockingDeque,BlockingQueue,
BrokenBarrierException,Callable,CancellationException,CompletableFuture,
CompletionException,CompletionService,CompletionStage,ConcurrentHashMap,
ConcurrentLinkedDeque,ConcurrentLinkedQueue,ConcurrentMap,ConcurrentNavigableMap,
ConcurrentSkipListMap,ConcurrentSkipListSet,CopyOnWriteArrayList,CopyOnWriteArraySet,
CountDownLatch,CounterCompleter,CyclicBarrier,Delayed,DelayQueue,Exchanger,
ExecutionException,Executor,ExecutorCompletionService,Executors,ExecutorService,
ForkJoinPool,ForkJoinTask,ForkJoinWorkerThread,Future,FutureTask,LinkedBlockingDeque,
LinkedBlockingQueue,LinkedTransferQueue,Phaser,PriorityBlockingQueue,RecursiveAction,
RecursiveTask,RejectedExecutionException,RejectedExecutionHandler,RunnableFuture,
RunnableScheduledFuture,ScheduledExecutorService,ScheduledThreadPoolExecutor,
Semaphore,SynchronousQueue,ThreadFactory,ThreadLocalRandom,ThreadPoolExecutor,
TimeoutException,TimeUnit,TransferQueue
java.util.concurrent.atomic包下：
AtomicBoolean,AtomicInteger,AtomicIntegerArray,AtomicIntegerFieldUpdater,
AtomicLong,AtomicLongArray,AtomicLongFieldUpdater,AtomicMarkableReference,
AtomicReference,AtomicReferenceArray,AtomicReferenceFieldUpdater,
AtomicStampedReference,DoubleAccumulator,DoubleAdder,LongAccumulator,
LongAdder,Striped64
java.util.concurrent.locks包下：
AbstractOwnableSynchronizer,AbstractQueuedLongSynchronizer,
AbstractQueuedSynchronizer,Condition,Lock,LockSupport,ReadWriteLock,
ReentrantLock,ReentrantReadWrtieLock,StampedLock

public class ConcurrentHashMap<K, V> extends AbstractMap<K, V> 
	implements ConcurrentMap<K, V>, Serializable {
	...
	//Encodings for Node hash fields. 
	static final int MOVED     = -1; //hash for forwarding nodes 
	static final int TREEBIN   = -2; //hash for roots of trees
	static final int RESERVED  = -3; //hash for transient reservations
	static final int HASH_BITS = 0x7fffffff; // usable bits of normal node hash 
	...
	/* ----------------- Table element access -------------- */
	/* Volatile access methods are used for table elements as well as 
		elements of in-process next table while resizing. All uses of 
		the tab arguments must be null checked by callers. All callers 
		also paranoically precheck that tab's length is not zero (or an 
		equivalent check), thus ensuring that any index argument taking 
		the form of a hash value anded with (length-1) is a valid 
		index. Note that, to be correct write arbitrary concurrency 
		errors by users, these checks must operate on local variables,
		which accounts for some odd-looking inline assignments below.
		Note that calls to setTabAt always occur within locked regions,
		and so in principle require only release ordering, not full volatile 
		semantics, but are currently coded as volatile writes to be conservative.
	*/
	
	@SuppressWarnings("unchecked")
	static final <K, V> Node<K, V> tabAt(Node<K,V>[] tab, int i) {
		return (Node<K,V>) U.getObjectVolatile(tab, ((long)i << ASHIFT) + ABASE);
	}
	static final <K,V> boolean casTabAt(Node<K,V>[] tab, int i, 
									Node<K,V> c, Node<K,V> v) {
		return U.compareAndSwapObject(tab, ((long)i << ASHIFT) + ABASE, c, v);
	}
	static final <K,V> void setTabAt(Node<K,V> tab, int i, Node<K,V> v) {
		U.putObjectVolatile(tab, ((long)i << ASHIFT) + ABASE, v);
	}
	
	/* The array of bins. Lazily initialized upon first insertion.
		Size always a power of two. Accessed directly by iterators.
	*/
	transient volatile Node<K,V>[] table;
	// The next table to use; non-null only while resizing.
	private transient volatile Node<K,V>[] nextTable;
	/* Base counter value, used mainly when there is no contention,
	 but also as a fallback during table initialization races. 
	 Updated via CAS.
	*/
	private transient volatile long baseCount;
	/* Table initialization and resizing control. When negative, the 
		table is being initialized or resized: -1 for initialization, 
		else -(1 + the number of active resizing threads). Otherwise,
		when table is null, holds the initial table size to use upon 
		creation, or 0 for default. After initialization, holds the 
		next element count value upon which to resize the table.
	*/
	private transient volatile int sizeCtl;
	// The next table index(plus one) to split while resizing.
	private transient volatile int transferIndex;
	//Spinlock(locked via CAS) used when resizing and/or creating CounterCells.
	private transient volatile int cellsBusy;
	//Table of counter cells. When non-null, size is a power of 2.
	private transient volatile CounterCell[] counterCells;
	//views
	private transient KeySetView<K,V> keySet;
	private transient ValuesView<K,V> values;
	private transient EntrySetView<K,V> entrySet;
	...
	/* Spends (XORs) higher bits of hash to lower and also forces top 
		bit to 0. Because the table uses a power-of-two masking, sets of 
		hashes that vary only in bits above the current mask will always collide.
		(Among known examples are sets of Float keys holding consecutive whole
		numbers in small tables.) So we apply a transform that spreads the impact 
		of higher bits downward. There is a tradeoff between speed, utility, and 
		quality of bit-spreading. Because many common sets of hashes are already 
		reasonably distributed (so don't benefit from spreading), and because 
		we trees to handle large sets of collisions in bins, we just XOR some 
		shifted bits in the cheapest possible way to reduce systematic lossage, 
		as well as to incorporate impact of the highest bits that would otherwise 
		never be used in index calculations because for table bounds.
	*/
	static final int spread(int h) {
		return (h ^ (h >>> 16)) & HASH_BITS;
	}
	// Initializes table, using the size recorded in sizeCtl.
	private final Node<K,V>[] initTable() {
		Node<K,V>[] tab; int sc;
		while ((tab = table) == null || tab.length == 0) {
			if ((sc = sizeCtl) < 0) 
				Thread.yield();  //lost initialization race; just spin
			else if (U.compareAndSwapInt(this, SIZECTL, sc, -1) {
				try {
					if ((tab = table) == null || tab.length == 0) {
						int n = (sc > 0) ? sc : DEFAULT_CAPACITY;
						@SuppressWarnings("unchecked")
                        Node<K,V>[] nt = (Node<K,V>[])new Node<?,?>[n];
                        table = tab = nt;
                        sc = n - (n >>> 2);
					}
				} finally {
					sizeCtl = sc;
				}
				break;
			}
		}
		return tab;
	}
	
	//Helps transfer if a resize is in process.
	final Node<K,V> helpTransfer(Node<K,V> tab, Node<K,V> f) {
		Node<K,V>[] nextTab; int sc;
		if (tab != null && (f instanceof ForwardingNode) && 
			(nextTab = ((ForwardingNode<K,V>)f).nextTable) != null) {
			int rs = resizeStamp(tab.length);
			while (nextTab == nextTable && table == tab && (sc = sizeCtl) < 0) {
				if ((sc >>> RESIZE_STAMP_SHIFT) != rs || sc == rs + 1 || 
					sc == rs + MAX_RESIZERS || transferIndex <= 0) {
					break;
				}
				if (U.compareAndSwapInt(this, SIZECTL, sc, sc + 1)) {
					transfer(tab, nextTab);
					break;
				}
			}
		}
		return table;
	}
	
	/* Adds to count, and if table is too small and not already resizing, 
		initiates transfer. If already resizing, helps perform transfer 
		if work is available. Rechecks occupancy after a transfer to see 
		if another resize is already needed because resizings are lagging additions.
	*/
    private final void addCount(long x, int check) {
        CounterCell[] as; long b, s;
        if ((as = counterCells) != null ||
            !U.compareAndSwapLong(this, BASECOUNT, b = baseCount, s = b + x)) {
            CounterCell a; long v; int m;
            boolean uncontended = true;
            if (as == null || (m = as.length - 1) < 0 ||
                (a = as[ThreadLocalRandom.getProbe() & m]) == null ||
                !(uncontended =
                  U.compareAndSwapLong(a, CELLVALUE, v = a.value, v + x))) {
                fullAddCount(x, uncontended);
                return;
            }
            if (check <= 1)
                return;
            s = sumCount();
        }
        if (check >= 0) {
            Node<K,V>[] tab, nt; int n, sc;
            while (s >= (long)(sc = sizeCtl) && (tab = table) != null &&
                   (n = tab.length) < MAXIMUM_CAPACITY) {
                int rs = resizeStamp(n);
                if (sc < 0) {
                    if ((sc >>> RESIZE_STAMP_SHIFT) != rs || sc == rs + 1 ||
                        sc == rs + MAX_RESIZERS || (nt = nextTable) == null ||
                        transferIndex <= 0)
                        break;
                    if (U.compareAndSwapInt(this, SIZECTL, sc, sc + 1))
                        transfer(tab, nt);
                }
                else if (U.compareAndSwapInt(this, SIZECTL, sc,
                                             (rs << RESIZE_STAMP_SHIFT) + 2))
                    transfer(tab, null);
                s = sumCount();
            }
        }
    }
	...
	    /**
     * Maps the specified key to the specified value in this table.
     * Neither the key nor the value can be null.
     *
     * <p>The value can be retrieved by calling the {@code get} method
     * with a key that is equal to the original key.
     *
     * @param key key with which the specified value is to be associated
     * @param value value to be associated with the specified key
     * @return the previous value associated with {@code key}, or
     *         {@code null} if there was no mapping for {@code key}
     * @throws NullPointerException if the specified key or value is null
     */
    public V put(K key, V value) {
        return putVal(key, value, false);
    }

    /** Implementation for put and putIfAbsent */
    final V putVal(K key, V value, boolean onlyIfAbsent) {
        if (key == null || value == null) throw new NullPointerException();
        int hash = spread(key.hashCode());
        int binCount = 0;
        for (Node<K,V>[] tab = table;;) {
            Node<K,V> f; int n, i, fh;
            if (tab == null || (n = tab.length) == 0)
                tab = initTable();
            else if ((f = tabAt(tab, i = (n - 1) & hash)) == null) {
                if (casTabAt(tab, i, null,
                             new Node<K,V>(hash, key, value, null)))
                    break;                   // no lock when adding to empty bin
            }
            else if ((fh = f.hash) == MOVED)
                tab = helpTransfer(tab, f);
            else {
                V oldVal = null;
                synchronized (f) {
                    if (tabAt(tab, i) == f) {
                        if (fh >= 0) {
                            binCount = 1;
                            for (Node<K,V> e = f;; ++binCount) {
                                K ek;
                                if (e.hash == hash &&
                                    ((ek = e.key) == key ||
                                     (ek != null && key.equals(ek)))) {
                                    oldVal = e.val;
                                    if (!onlyIfAbsent)
                                        e.val = value;
                                    break;
                                }
                                Node<K,V> pred = e;
                                if ((e = e.next) == null) {
                                    pred.next = new Node<K,V>(hash, key,
                                                              value, null);
                                    break;
                                }
                            }
                        }
                        else if (f instanceof TreeBin) {
                            Node<K,V> p;
                            binCount = 2;
                            if ((p = ((TreeBin<K,V>)f).putTreeVal(hash, key,
                                                           value)) != null) {
                                oldVal = p.val;
                                if (!onlyIfAbsent)
                                    p.val = value;
                            }
                        }
                    }
                }
                if (binCount != 0) {
                    if (binCount >= TREEIFY_THRESHOLD)
                        treeifyBin(tab, i);
                    if (oldVal != null)
                        return oldVal;
                    break;
                }
            }
        }
        addCount(1L, binCount);
        return null;
    }
	...
	/* Minimum number of rebinnings per transfer step. Ranges are 
		subdivided to allow multiple resizer threads. This value 
		servers as a low bound to avoid resizers encountering 
		execssive memory contention. The value should be at least DEFAULT_CAPACITY.
	*/
	private static final int MIN_TRANSFER_STRIDE = 16;
	/* The smallest table capacity for which bins may be treeified.
		(Otherwise the table is resized if too many nodes in a bin.)
		The value should be at 4 * TREEIFY_THRESHOLD to avoid conflicts 
		between resizing and treeification thresholds.
	*/
	static final int MIN_TREEIFY_CAPACITY = 64;
	...
	
}

可以发现JDK8中ConcurrentHashMap的实现使用的是锁分离思想，只是锁住一个Node，而锁住Node之前的操作是基于volatile和CAS之上无锁并且线程安全的。

什么条件下会进行技术桶的扩容？
答：在CAS操作递增计数桶失败了3次之后，会进行扩容计数桶操作，注意此时同时进行了两次随机定位计数桶来进行CAS递增的，所以此时可以保证大概率是因为计数桶不够用了，才会进行计数桶扩容。
计数桶扩容操作是怎么样的？
答：计数桶长度增加到两倍长度，数据直接遍历迁移过来，由于计数桶不像HashMap数据结构那么复杂，有hash算法的影响，加上计数桶只是存放long类型的计数值而已，所以直接赋值引用即可。

ConcurrentHashMap中计数用到的并发技巧：	
	1. 利用CAS递增baseCount值来感知是否存在线程竞争，若竞争不大直接CAS递增baseCount值即可，性能和baseCount++差不多。
	2. 若存在线程竞争，则初始化计数桶，若此时初始化计数桶的过程中也存在竞争，多个线程同时初始化计数桶，则没有强盗初始化资格的线程直接尝试CAS递增baseCount值的方式完成计数，最大化利用线程的并行。此时使用计数桶计数，分而治之的方式计数，此时两个计数桶最大可提供两个线程同时计数，同时使用CAS操作来感知线程竞争，若两个桶情况下CAS操作还是频繁失败(失败3次)，则直接扩容计数桶，变成4个计数桶，支持最大4个线程并发计数，以此类推...同时使用位运算和随机数的方式"负载均衡"一样的将线程计数请求接近平均的落在每个计数桶中。
对于ConcurrentHashMap#get()，其实没有线程安全问题，只有可见性的问题，只需要确保get()的数据是线程之间可见的即可。

其中1.7的实现也同样采用了分段锁的技术，只不过多个一个segment，一个segment里对应一个小HashMap，其中segment继承了ReentrantLock，充当了锁的角色，一把锁锁一个小HashMap（相当于多个Node），从1.8的实现来看， 锁的粒度从多个Node级别又减小到一个Node级别，再度减小锁竞争，减小程序同步的部分。



	

 
 
 
 